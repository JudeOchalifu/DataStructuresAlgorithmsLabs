{\rtf1\ansi\ansicpg1252\cocoartf1138\cocoasubrtf510
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 \
Thought Questions 2.11\
\
If we use an ordered vector, the vector would contain complete words instead of word paths.  Based on the examples in the lab handout, I assumed that the suggested corrections must be the same length as the input word.  Thus, in order to suggest correction, we would have a recursive call with an index parameter.  This index parameter would allow us to extract the given character at the given index.  We would use that index to search through every single word in the order vector to see if the character at that same index is the same or different.  If it is different, our recursive call would decrease the value of maxDistance and if that value becomes less than 0, we would stop the given recursive call.  In order to avoid useless suggestion, i.e. suggesting a 4 letter word for a 3 letter input word, we could check the given word's length in the ordered vector every time to avoid these inefficiencies.  Even if we assume that past a certain point, all the words starting with letter a for example exceed the three length input, we'd still have to check every single element in the vector to see when we reach the b starting words, etc.  This approach would therefore be extremely inefficient as every single iteration would have to check all the elements of the vector thus for a word of length k and a ordered vector of length n, the total time would roughly be k* n.  Our current approach using a LexiconTrie is significantly more efficient, as we would only care about the children of a given node.  The total time would then be k * avg number of children / node.  We know that avg children per node is significantly less than n or total number of words in the trie/ordered Vector.  }